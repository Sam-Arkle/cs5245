Notes on machine learning homework:

In general approach with naive Bayes and see if you can top it. It is a nice quick and easy model to apply.

Challenge 1:

From the comparison it looks like Naive bayes is a good start. Let's investigate Gaussian process, seems to be succesful. 
default is:     GaussianProcessClassifier(1.0 * RBF(1.0)),


Gaussian process can be seen as improving on K-nearest neighbour. K nearest is always circular. Not always the ideal shape. 

Book wasn't necessarily the bigget help in getting to understand this particular model... 

Best Config: {'kernel': 1**2 * RBF(length_scale=1)}
gaussian process results on challenge one

So hyperparam tweaking suggests the best is the default values

Challenge 2:

Dataset 2 looks very similar to data set 1, but gets fairly different results on naive bayes. 

QDA got a score of 0.9. 
Naive bayes 0.84375

Why?

Likely because QDA allows for quadratic boundaries. Which gives some more flexibility with this slightly more skewed dataset.
No hyperparam tweaking possible on this one?

Challenge 3:

The data sets looks slightly more seperated on this but it doesn't work at all well with naive bayes
results are 0.56875 for naive
Neural net and QDA both do very well. Let's learn about neural net and then we can easilt apply the QDE.
Neural net implemented through a multi layer perceptron
The advantages of Multi-layer Perceptron are:

        Capability to learn non-linear models.

        Capability to learn models in real-time (on-line learning) using partial_fit.

The disadvantages of Multi-layer Perceptron (MLP) include:

        MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.

        MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.

        MLP is sensitive to feature scaling.
	
It is worth scaling the data for using MLP. So will try giving that a whizz too!

With alpha 1 we get a result of 0.9125. The alpha appears to alter the softness of the decision boundaries. https://scikit-learn.org/stable/modules/neural_networks_supervised.html

alpha 0.32: 0.9
0.5: 0.8875
0.75: 0.90625
Same for 1.25

Scaling the data we get a slightly improved result of 0.91875

QDA also yields 0.91875 so we'll stick with that for now as we won't be able to scale the data for the webcat submission...

Challenge 4
Dataset on this one is much more intermingled. There is a clear distribution but the centre ground is very vague.

QDA seems to do well here
As does neural net
Naive Bayes: 0.83125
QDA: 0.8625

For gaussian process best initial seems to be 0.842 with 'kernel': 1**2 * Matern(length_scale=1, nu=1.5)

comes out with 0.85625 with model = GaussianProcessClassifier(1 * Matern())

Neural net alpha 1: 0.8375
alpha 3.2 looks promising...
3.2 alpha yields 0.8625

Went with neural net and got a score of .82 on webcat...

Will try submitting now with QDA
also .82 so let's just stick with neural net


Challenge 5
Similar to challenge 4 but a bit more densely packed by the looks of things...

NaiveBayes: 0.80625
According to our charts RBF SVM should be a winner here.
Result is 0.88125 using the params taken from the code. SVC(gamma=2, C=1)

SVMs are support vector machines
line of best fit but with margins on either side seems to be how the simple linear one works. 
rbf stands for radial basis function and we get radii rather than lines. We can soften this margin through the use of c

Mcuking about with c values:
c:0.5: accuracy 0.86875
c:0.2: accuracy 0.875

c= 0.2 gamma 1 seems to be the winner

this gets us 0.85625...

Webcat gave a 90 so we'll sticck with that for now...

Challenge 6:
This is the cool spiral data so a line of best fit seems very innapropriate

Most of the classifiers did quite well here, but nearest neighbour (3) got 100 so it seems a good place to start
naive bayes: Accuraccy score is:  0.60625
QDA: Accuraccy score is:  0.59375
Nearest neighbour got a full on 1.0! This is with k = 3
We could do a similar grid search here to optimise? But given we have a strong result already, might move on and focus on the harder problems.

Challenge 7:
Data looks very messy on this one. Roughly two horizontal stripes overlapping two vertical stripes.
None of the models do particularly well here, a coin toss seems to be the best way to label the data. 

Nearest neighbour has some success, so it might be worth trying to tweak on that one. Same for RBF svm. Maybe we will try that one first

naive bayes: Accuraccy score is:  0.525
K nearest with k = 5 Accuraccy score is:  0.6625


Got the grid search working for number of neighbours and it brings us to a pretty nice score of 75% on webcat when 16 neighbours are used.

Challenge 8:
The data appears to be very distinctly organised. Very little overlap. A rectangle with a big r shape making up the two patterns.

Gaussian, decision and adaboost all scored a solid 1.0 in the initial run so let's aim to match that.

Naive bayes gets 0.875
qda gets 0.8875

Let's learn about decision trees for this one.

Using DecisionTreeClassifier(max_depth=5) we get accuracy of 1.0.
Won't be able to tune on this one to check for accuracy improvements. But can still learn:

Random forests is an example of an ensemble method. relies on aggregating the results of an ensemble of simpler estimators.
Very easy to overfit the data by making the depth too large. This will be easiest to see visually so check online. 

random forest takes things a step further but we won't go into that just yet.

Using a grid to get the best depth we get 2. So we'll stick with that and submit to webcat.

Webcat gives us 100%

Challenge 9:
Seems to be pretty much the same data set as 8, but rotated. 
For some reason this appears to have messed up the decision tree method in the original run through. We should give it a spin though out of curiosity.
Radial SVM seems to be the best approach though so we'll try that too.

naive bayes: Accuraccy score is:  0.9375

Decision tree grid suggested a depth of 10, with that we get Accuraccy score is:  0.925
Clearly rotation has messed this one up

For the RBF SVM the grid suggests c=100 initially, which is getting 98.33 on the grid
model = SVC(C=100, gamma=1) Accuraccy score is:  0.95
model = SVC(gamma=2, C=1) Accuraccy score is:  0.96875

model = SVC(gamma=1, C=13) Accuraccy score is:  0.95 grid 98.75%

Not performing super well here, but let's put it to webcat to decide between them 

model = SVC(gamma=1, C=13) gets 98% on  webcat
model = SVC(gamma=2, C=1) gets 98.5% on webcat

So we'll stick with that

Challenge 10:
Fun shape on this one with some degree of overlap. Like a rotated tilda split longways for the data.

Initially it looks like GDA and neural net or gaussian process are going to be the best fits here.

model = GaussianNB() Accuraccy score is:  0.78125
model = QuadraticDiscriminantAnalysis() Accuraccy score is:  0.925
model = MLPClassifier(alpha=1, max_iter=1000) Accuraccy score is:  0.9125

The grid isn't very happy on this one, keep exceeding the iteration limit. Need to work out how to get that one happy.

Could also do with improving the gaussian process grid. 

model = GaussianProcessClassifier(1 * RBF()) Accuraccy score is:  0.925

We will go with Gaussian process on webcat
webcat scored it at 89%...
Let's try QDA... it also gets 89%
I'll leave it at that for now.

Challenge 11:
Similar to the previous one but with more overlap.
Original run suggests Neural net will be the best fit here but we know how tempermental that thing can be.
Next best is a tie between qda. nearest neighbour, gaussian process

model = GaussianNB() Accuraccy score is:  0.7625
model = QuadraticDiscriminantAnalysis() Accuraccy score is:  0.875
model = KNeighborsClassifier(3) Accuraccy score is:  0.89375
model = GaussianProcessClassifier(1 * RBF()) Accuraccy score is:  0.8625
model = GaussianProcessClassifier(1 * DotProduct()) Accuraccy score is:  0.925
model = MLPClassifier(alpha=1e-09, max_iter=1000) Accuraccy score is:  0.91875
model = MLPClassifier(alpha=0.01, max_iter=1000) Accuraccy score is:  0.89375

We'll try with Gaussian then.
get 93 on webcat


Challenge 12:
Cool data set. A donut shape mixed with its inverse. Not much overlap but the shapes fit inside one another. 
Unsurpsingly a strong performance from rbf SVM. Gaussian process does best and nearest neighbour is pretty good too. Let's try those three.

model = GaussianNB() Accuraccy score is:  0.53125
model = KNeighborsClassifier(3) Accuraccy score is:  0.8625
model = KNeighborsClassifier(1) Accuraccy score is:  0.86875
model = SVC(gamma=2, C=1) Accuracy score is:  0.7375
model = SVC(gamma=2, C=58) Accuracy score is:  0.9125
model = GaussianProcessClassifier(1 * DotProduct(sigma_0=1)) Accuraccy score is:  0.45
model = GaussianProcessClassifier(1 * RationalQuadratic()) Accuraccy score is:  0.9375

Final one there got a webcat score of 97% which seems pretty nifty.





====================

Have made a first pass at the challenges. 
Webccat scores are:

1: 93
2: 94.5
3: 92.5
4: 82.5
5: 90
6: 100
7: 75.5
8: 100
9: 98.5
10: 89
11: 93
12: 97


